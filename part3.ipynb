{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee96367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27a22d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Sampled Dataset Size: 852909 rows\n",
      "        type                                            content\n",
      "0       fake  The Mainstream Media has always been a Propaga...\n",
      "1       fake  Could Islam Destroy America?\\n\\nHeadline: Bitc...\n",
      "2       fake  SC barber, customers fire on, kill robbery sus...\n",
      "3       fake  Camping Bus\\n\\n% of readers think this story i...\n",
      "4  political  Take heart, Sanders supporters. Weâ€™re down, bu...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Increase field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Define chunk size and sampling ratio\n",
    "CHUNK_SIZE = 100000  # Read 100,000 rows at a time\n",
    "SAMPLE_RATIO = 0.10  # Extract 10% of total data\n",
    "\n",
    "sampled_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\"news.csv\", usecols=[\"content\", \"type\"], dtype=str, encoding=\"utf-8\",\n",
    "                         on_bad_lines=\"skip\", low_memory=True, chunksize=CHUNK_SIZE, engine=\"python\"):\n",
    "    chunk_sample = chunk.sample(frac=SAMPLE_RATIO, random_state=42)  # Sample 10% of each chunk\n",
    "    sampled_chunks.append(chunk_sample)\n",
    "\n",
    "# Combine all sampled chunks\n",
    "df_sampled = pd.concat(sampled_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Final Sampled Dataset Size: {len(df_sampled)} rows\")\n",
    "print(df_sampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6f0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to 'preprocessed_news.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Download required resources (first time only)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming df_sampled is already created with the 'content' and 'type' columns\n",
    "df_sampled.dropna(subset=[\"content\", \"type\"], inplace=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # Stem tokens\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    # Join back to text\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "# Function to apply preprocessing to a chunk of data\n",
    "def process_chunk(chunk):\n",
    "    chunk['processed_content'] = chunk['content'].apply(preprocess_text)\n",
    "    return chunk[['content', 'type', 'processed_content']]\n",
    "\n",
    "# Split the data into smaller chunks for parallel processing\n",
    "chunk_size = 50000  # Adjust this depending on memory constraints\n",
    "chunks = [df_sampled.iloc[i:i + chunk_size] for i in range(0, len(df_sampled), chunk_size)]\n",
    "\n",
    "# Use parallel processing to preprocess chunks\n",
    "processed_chunks = Parallel(n_jobs=-1)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "# Combine all processed chunks\n",
    "df_processed = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Save the processed data to a CSV file\n",
    "df_processed.to_csv(\"preprocessed_news.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Preprocessed data saved to 'preprocessed_news.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b045de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: ['fake' 'political' 'conspiracy' 'bias' 'satire' 'clickbait' 'junksci'\n",
      " 'unreliable' 'hate' 'unknown' 'rumor' 'reliable']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% temp\n",
    "train_df, temp_df = train_test_split(df_processed, test_size=0.2, stratify=df_processed[\"type\"], random_state=42,train_size=0.8)\n",
    "# Split temp into 50% validation, 50% test (10% each of total)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"type\"], random_state=42)\n",
    "\n",
    "print(\"Unique labels in the dataset:\", df_processed['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e940c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train NaN check:\n",
      "Content: 0\n",
      "Labels: 0\n",
      "\n",
      "Validation NaN check:\n",
      "Content: 0\n",
      "Labels: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.88      0.99      0.93     62151\n",
      "    reliable       0.96      0.55      0.70     19120\n",
      "\n",
      "    accuracy                           0.89     81271\n",
      "   macro avg       0.92      0.77      0.81     81271\n",
      "weighted avg       0.90      0.89      0.88     81271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example mapping (adjusted based on actual labels)\n",
    "label_mapping = {\n",
    "    'fake': 'fake',\n",
    "    'political': 'fake',\n",
    "    'conspiracy': 'fake',\n",
    "    'bias': 'fake',\n",
    "    'satire': 'fake',\n",
    "    'clickbait': 'fake',\n",
    "    'junksci': 'fake',\n",
    "    'unreliable': 'fake',\n",
    "    'hate': 'fake',\n",
    "    'unknown': 'fake',\n",
    "    'rumor': 'fake',\n",
    "    'reliable': 'reliable'\n",
    "}\n",
    "df_processed['binary_label'] = df_processed['type'].map(label_mapping)\n",
    "train_df['binary_label'] = train_df['type'].map(label_mapping)\n",
    "val_df['binary_label'] = val_df['type'].map(label_mapping)\n",
    "\n",
    "print(\"Train NaN check:\")\n",
    "print(f\"Content: {train_df['content'].isna().sum()}\")\n",
    "print(f\"Labels: {train_df['binary_label'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nValidation NaN check:\")\n",
    "print(f\"Content: {val_df['content'].isna().sum()}\")\n",
    "print(f\"Labels: {val_df['binary_label'].isna().sum()}\")\n",
    "\n",
    "# Create pipeline\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000)),  # Reduce dimensionality\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.fit(train_df['content'], train_df['binary_label'])\n",
    "\n",
    "y_pred = model.predict(val_df['content'])\n",
    "print(classification_report(val_df['binary_label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf9f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_count  char_count  avg_word_length\n",
      "0        1245        7976         6.406426\n",
      "1          81         497         6.135802\n",
      "2         151        1023         6.774834\n",
      "3         177        1040         5.875706\n",
      "4         959        5658         5.899896\n"
     ]
    }
   ],
   "source": [
    "# Create simple meta-data features\n",
    "df_processed['word_count'] = df_processed['content'].apply(lambda x: len(str(x).split()))\n",
    "df_processed['char_count'] = df_processed['content'].apply(lambda x: len(str(x)))\n",
    "df_processed['avg_word_length'] = df_processed['char_count'] / df_processed['word_count']\n",
    "\n",
    "# Check the result\n",
    "print(df_processed[['word_count', 'char_count', 'avg_word_length']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e1c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word_count  char_count  avg_word_length\n",
      "447564          32         213         6.656250\n",
      "788648         643        3834         5.962675\n",
      "524854        1379        8513         6.173314\n",
      "684620         107         637         5.953271\n",
      "164967         812        4992         6.147783\n",
      "        word_count  char_count  avg_word_length\n",
      "515684         179        1047         5.849162\n",
      "742404         224        1418         6.330357\n",
      "806525         916        5613         6.127729\n",
      "326627          19         128         6.736842\n",
      "132704         338        2158         6.384615\n"
     ]
    }
   ],
   "source": [
    "train_df['word_count'] = train_df['content'].apply(lambda x: len(str(x).split()))\n",
    "train_df['char_count'] = train_df['content'].apply(lambda x: len(str(x)))\n",
    "train_df['avg_word_length'] = train_df['char_count'] / train_df['word_count']\n",
    "\n",
    "print(train_df[['word_count', 'char_count', 'avg_word_length']].head())\n",
    "\n",
    "\n",
    "val_df['word_count'] = val_df['content'].apply(lambda x: len(str(x).split()))\n",
    "val_df['char_count'] = val_df['content'].apply(lambda x: len(str(x)))\n",
    "val_df['avg_word_length'] = val_df['char_count'] / val_df['word_count']\n",
    "print(val_df[['word_count', 'char_count', 'avg_word_length']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bc3cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.94      0.97      0.95     62151\n",
      "    reliable       0.89      0.79      0.84     19120\n",
      "\n",
      "    accuracy                           0.93     81271\n",
      "   macro avg       0.92      0.88      0.90     81271\n",
      "weighted avg       0.93      0.93      0.93     81271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Meta-data features\n",
    "for df in [train_df, val_df]:\n",
    "    df['word_count'] = df['content'].apply(lambda x: len(str(x).split()))\n",
    "    df['char_count'] = df['content'].apply(lambda x: len(str(x)))\n",
    "    df['avg_word_length'] = df['char_count'] / (df['word_count'] + 1e-5)\n",
    "\n",
    "X_train = train_df[['content', 'word_count', 'char_count', 'avg_word_length']]\n",
    "X_val = val_df[['content', 'word_count', 'char_count', 'avg_word_length']]\n",
    "y_train = train_df['binary_label']\n",
    "y_val = val_df['binary_label']\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('text', TfidfVectorizer(max_features=1000), 'content'),\n",
    "    ('meta', StandardScaler(), ['word_count', 'char_count', 'avg_word_length'])\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=500, solver='liblinear', random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
