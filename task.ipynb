{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Increase field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Define chunk size and sampling ratio\n",
    "CHUNK_SIZE = 100000  # Read 100,000 rows at a time\n",
    "SAMPLE_RATIO = 0.10  # Extract 10% of total data\n",
    "\n",
    "sampled_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\"news.csv\", usecols=[\"content\", \"type\"], dtype=str, encoding=\"utf-8\",\n",
    "                         on_bad_lines=\"skip\", low_memory=True, chunksize=CHUNK_SIZE, engine=\"python\"):\n",
    "    chunk_sample = chunk.sample(frac=SAMPLE_RATIO, random_state=42)  # Sample 10% of each chunk\n",
    "    sampled_chunks.append(chunk_sample)\n",
    "\n",
    "# Combine all sampled chunks\n",
    "df_sampled = pd.concat(sampled_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Final Sampled Dataset Size: {len(df_sampled)} rows\")\n",
    "print(df_sampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Download required resources (first time only)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming df_sampled is already created with the 'content' and 'type' columns\n",
    "df_sampled.dropna(subset=[\"content\", \"type\"], inplace=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # Stem tokens\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    # Join back to text\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "# Function to apply preprocessing to a chunk of data\n",
    "def process_chunk(chunk):\n",
    "    chunk['processed_content'] = chunk['content'].apply(preprocess_text)\n",
    "    return chunk[['content', 'type', 'processed_content']]\n",
    "\n",
    "# Split the data into smaller chunks for parallel processing\n",
    "chunk_size = 50000  # Adjust this depending on memory constraints\n",
    "chunks = [df_sampled.iloc[i:i + chunk_size] for i in range(0, len(df_sampled), chunk_size)]\n",
    "\n",
    "# Use parallel processing to preprocess chunks\n",
    "processed_chunks = Parallel(n_jobs=-1)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "# Combine all processed chunks\n",
    "df_processed = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Save the processed data to a CSV file\n",
    "df_processed.to_csv(\"preprocessed_news.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Preprocessed data saved to 'preprocessed_news.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
