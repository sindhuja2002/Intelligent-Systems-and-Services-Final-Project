{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Sampled Dataset Size: 852909 rows\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Increase field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Define chunk size and sampling ratio\n",
    "CHUNK_SIZE = 100000  # Read 100,000 rows at a time\n",
    "SAMPLE_RATIO = 0.10  # Extract 10% of total data\n",
    "\n",
    "sampled_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\"news.csv\", usecols=[\"content\"], dtype=str, encoding=\"utf-8\",\n",
    "                         on_bad_lines=\"skip\", low_memory=True, chunksize=CHUNK_SIZE, engine=\"python\"):\n",
    "    chunk_sample = chunk.sample(frac=SAMPLE_RATIO, random_state=42)  # Sample 10% of each chunk\n",
    "    sampled_chunks.append(chunk_sample)\n",
    "\n",
    "# Combine all sampled chunks\n",
    "df_sampled = pd.concat(sampled_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"✅ Final Sampled Dataset Size: {len(df_sampled)} rows\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Initial Vocabulary Size: 1116207\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 🔹 Function to Compute Vocabulary Size in Chunks\n",
    "def get_vocabulary_size(text_series, chunk_size=5000):\n",
    "    vocab = Counter()  # Counter to store word frequencies\n",
    "    for i in range(0, len(text_series), chunk_size):\n",
    "        chunk_tokens = []\n",
    "        \n",
    "        # Tokenize each text in the chunk\n",
    "        for text in text_series[i:i + chunk_size]:\n",
    "            if text:  # Check if the text is not None or empty\n",
    "                tokens = word_tokenize(text)  # Tokenize each text\n",
    "                tokens = [token.lower() for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens and convert to lowercase\n",
    "                chunk_tokens.extend(tokens)  # Add tokens to the chunk_tokens list\n",
    "        \n",
    "        # Update vocabulary with tokens from this chunk\n",
    "        vocab.update(chunk_tokens)\n",
    "    \n",
    "    return len(vocab)  # Return the vocabulary size\n",
    "\n",
    "# Compute initial vocabulary size (before processing)\n",
    "vocab_size_before = get_vocabulary_size(df_sampled['content'])\n",
    "\n",
    "print(f\"📝 Initial Vocabulary Size: {vocab_size_before}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 🔹 Remove null values from the 'content' column\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_sampled = \u001b[43mdf_sampled\u001b[49m.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 🔹 Function to Preprocess Text: Remove stopwords and apply stemming\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_text\u001b[39m(text):\n",
      "\u001b[31mNameError\u001b[39m: name 'df_sampled' is not defined"
     ]
    }
   ],
   "source": [
    "# 🔹 Remove null values from the 'content' column\n",
    "df_sampled = df_sampled.dropna(subset=['content'])\n",
    "\n",
    "# 🔹 Function to Preprocess Text: Remove stopwords and apply stemming\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens and convert to lowercase\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming\n",
    "    return tokens\n",
    "\n",
    "# 🔹 Function to Compute Vocabulary Size After Preprocessing\n",
    "def get_vocabulary_size_processed(text_series, chunk_size=5000):\n",
    "    vocab = Counter()\n",
    "    for i in range(0, len(text_series), chunk_size):\n",
    "        chunk_tokens = [preprocess_text(text) for text in text_series[i:i + chunk_size]]\n",
    "        for tokens in chunk_tokens:\n",
    "            vocab.update(tokens)\n",
    "    return len(vocab)\n",
    "\n",
    "# Compute vocabulary size after preprocessing (removing stopwords and stemming)\n",
    "vocab_size_after = get_vocabulary_size_processed(df_sampled['content'])\n",
    "\n",
    "print(f\"📝 Vocabulary Size After Preprocessing: {vocab_size_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(re.findall(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttp[s]?://\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+\u001b[39m\u001b[33m'\u001b[39m, text))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Count URLs in the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df_sampled[\u001b[33m'\u001b[39m\u001b[33murl_count\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_sampled\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Check some stats about URL counts\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_sampled[\u001b[33m'\u001b[39m\u001b[33murl_count\u001b[39m\u001b[33m'\u001b[39m].describe())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcount_urls\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount_urls\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp[s]?://\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mS+\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:278\u001b[39m, in \u001b[36mfindall\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfindall\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[32m    272\u001b[39m \n\u001b[32m    273\u001b[39m \u001b[33;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \n\u001b[32m    277\u001b[39m \u001b[33;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'NoneType'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to count URLs\n",
    "def count_urls(text):\n",
    "    return len(re.findall(r'http[s]?://\\S+', text))\n",
    "\n",
    "# Count URLs in the dataset\n",
    "df_sampled['url_count'] = df_sampled['content'].apply(count_urls)\n",
    "\n",
    "# Check some stats about URL counts\n",
    "print(df_sampled['url_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    852895.000000\n",
      "mean          0.050565\n",
      "std           1.190655\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max         376.000000\n",
      "Name: date_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to count dates\n",
    "def count_dates(text):\n",
    "    return len(re.findall(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', text))\n",
    "\n",
    "# Count dates in the dataset\n",
    "df_sampled['date_count'] = df_sampled['content'].apply(count_dates)\n",
    "\n",
    "# Check some stats about date counts\n",
    "print(df_sampled['date_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    852895.000000\n",
      "mean          9.771909\n",
      "std          31.807210\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           3.000000\n",
      "75%          10.000000\n",
      "max        3361.000000\n",
      "Name: num_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to count numeric values\n",
    "def count_numbers(text):\n",
    "    return len(re.findall(r'\\b\\d+\\b', text))\n",
    "\n",
    "# Count numbers in the dataset\n",
    "df_sampled['num_count'] = df_sampled['content'].apply(count_numbers)\n",
    "\n",
    "# Check some stats about numeric counts\n",
    "print(df_sampled['num_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'url_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'url_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m total_urls = \u001b[43mdf_sampled\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl_count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.sum()\n\u001b[32m      2\u001b[39m total_dates = df_sampled[\u001b[33m\"\u001b[39m\u001b[33mdate_count\u001b[39m\u001b[33m\"\u001b[39m].sum()\n\u001b[32m      3\u001b[39m total_numbers = df_sampled[\u001b[33m\"\u001b[39m\u001b[33mnum_count\u001b[39m\u001b[33m\"\u001b[39m].sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'url_count'"
     ]
    }
   ],
   "source": [
    "total_urls = df_sampled[\"url_count\"].sum()\n",
    "total_dates = df_sampled[\"date_count\"].sum()\n",
    "total_numbers = df_sampled[\"num_count\"].sum()\n",
    "\n",
    "print(f\"Total URLs: {total_urls}\")\n",
    "print(f\"Total Dates: {total_dates}\")\n",
    "print(f\"Total Numbers: {total_numbers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Increase field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Define chunk size and sampling ratio\n",
    "CHUNK_SIZE = 100000  # Read 100,000 rows at a time\n",
    "SAMPLE_RATIO = 0.10  # Extract 10% of total data\n",
    "\n",
    "sampled_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\"news.csv\", usecols=[\"content\"], dtype=str, encoding=\"utf-8\",\n",
    "                         on_bad_lines=\"skip\", low_memory=True, chunksize=CHUNK_SIZE, engine=\"python\"):\n",
    "    chunk_sample = chunk.sample(frac=SAMPLE_RATIO, random_state=42)  # Sample 10% of each chunk\n",
    "    sampled_chunks.append(chunk_sample)\n",
    "\n",
    "# Combine all sampled chunks\n",
    "df_sampled = pd.concat(sampled_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"✅ Final Sampled Dataset Size: {len(df_sampled)} rows\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_sampled), chunk_size):\n\u001b[32m     12\u001b[39m     chunk = df_sampled[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m][i : i + chunk_size].dropna()  \u001b[38;5;66;03m# Avoid null values\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     chunk_tokens = [\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m chunk]  \u001b[38;5;66;03m# Tokenize each text\u001b[39;00m\n\u001b[32m     14\u001b[39m     chunk_tokens = [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m chunk_tokens \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]  \u001b[38;5;66;03m# Flatten list\u001b[39;00m\n\u001b[32m     15\u001b[39m     freq_dist_before.update(chunk_tokens)  \u001b[38;5;66;03m# Update frequency distribution\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:127\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Standard word tokenizer.\u001b[39;00m\n\u001b[32m    124\u001b[39m _treebank_word_tokenizer = NLTKWordTokenizer()\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize an empty frequency distribution\n",
    "freq_dist_before = FreqDist()\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000  # Adjust based on memory constraints\n",
    "\n",
    "# Process in chunks\n",
    "for i in range(0, len(df_sampled), chunk_size):\n",
    "    chunk = df_sampled[\"content\"][i : i + chunk_size].dropna()  # Avoid null values\n",
    "    chunk_tokens = [word_tokenize(text.lower()) for text in chunk]  # Tokenize each text\n",
    "    chunk_tokens = [item for sublist in chunk_tokens for item in sublist]  # Flatten list\n",
    "    freq_dist_before.update(chunk_tokens)  # Update frequency distribution\n",
    "\n",
    "# Get the 100 most frequent words\n",
    "most_frequent_words = freq_dist_before.most_common(100)\n",
    "\n",
    "print(most_frequent_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 1218535), ('one', 957890), ('new', 908562), ('would', 836596), ('time', 827155), ('state', 801855), ('year', 783373), ('peopl', 755543), ('like', 720122), ('use', 604798), ('also', 592218), ('us', 557397), ('make', 533384), ('say', 512913), ('go', 500536), ('get', 496334), ('even', 488537), ('presid', 458326), ('govern', 458183), ('american', 445926), ('work', 444421), ('report', 439072), ('right', 435436), ('nation', 430620), ('could', 430451), ('first', 427957), ('two', 423061), ('day', 421740), ('trump', 419425), ('news', 418016), ('mani', 417934), ('come', 416948), ('obama', 410792), ('continu', 391564), ('world', 389418), ('take', 389185), ('stori', 386862), ('may', 383698), ('read', 378005), ('call', 373205), ('last', 373199), ('want', 363934), ('see', 363062), ('includ', 354478), ('know', 352014), ('think', 351412), ('way', 349348), ('countri', 341515), ('back', 339537), ('need', 334487), ('public', 332417), ('support', 326465), ('polit', 319836), ('unit', 310616), ('tri', 307897), ('look', 307813), ('iran', 305078), ('york', 303129), ('hous', 301415), ('show', 300949), ('war', 299828), ('well', 298012), ('much', 292222), ('chang', 290572), ('system', 285095), ('pleas', 276726), ('live', 276267), ('group', 274678), ('thing', 274241), ('help', 273630), ('law', 267043), ('made', 266413), ('good', 259007), ('week', 258687), ('compani', 256821), ('million', 255703), ('issu', 252140), ('democrat', 251570), ('sinc', 250728), ('republican', 248436), ('sign', 248349), ('inform', 246375), ('point', 241989), ('still', 241077), ('part', 240791), ('health', 240659), ('end', 240566), ('power', 240471), ('offic', 240304), ('plan', 236829), ('leader', 233623), ('main', 233113), ('servic', 231444), ('citi', 229592), ('forc', 229342), ('anoth', 227870), ('attack', 227820), ('percent', 227770), ('gener', 227107), ('parti', 224881)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Load English stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Initialize frequency distribution and stemmer\n",
    "freq_dist = FreqDist()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000  \n",
    "\n",
    "# Process dataset in chunks\n",
    "for i in range(0, len(df_sampled), chunk_size):\n",
    "    chunk = df_sampled[\"content\"][i : i + chunk_size].dropna()  # Drop null values in the chunk\n",
    "    chunk_tokens = [word_tokenize(text.lower()) for text in chunk]  # Tokenize each text\n",
    "    \n",
    "    # Flatten the list of tokens\n",
    "    chunk_tokens = [item for sublist in chunk_tokens for item in sublist]  \n",
    "    \n",
    "    # Filter out stopwords, punctuation, and non-alphabetic tokens\n",
    "    filtered_tokens = [\n",
    "        stemmer.stem(word)  # Apply stemming\n",
    "        for word in chunk_tokens\n",
    "        if word.isalpha() and word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    \n",
    "    # Update frequency distribution\n",
    "    freq_dist.update(filtered_tokens)  \n",
    "\n",
    "# Get the 100 most frequent words after preprocessing\n",
    "most_frequent_words = freq_dist.most_common(100)\n",
    "\n",
    "# Print the 100 most frequent words\n",
    "print(most_frequent_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No words available for Top 30 Frequent Words (Before Preprocessing)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'freq_dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m plot_word_frequencies(freq_dist_before, \u001b[33m\"\u001b[39m\u001b[33mTop 30 Frequent Words (Before Preprocessing)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Plotting the 30 most frequent words after preprocessing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m plot_word_frequencies(\u001b[43mfreq_dist\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mTop 30 Frequent Words (After Preprocessing)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'freq_dist' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to safely plot word frequencies\n",
    "def plot_word_frequencies(freq_dist, title):\n",
    "    if not freq_dist or len(freq_dist) == 0:\n",
    "        print(f\"Warning: No words available for {title}\")\n",
    "        return\n",
    "    \n",
    "    # Get the top 30 most common words\n",
    "    most_common_words = freq_dist.most_common(30)\n",
    "    words, counts = zip(*most_common_words)  # Unzip words and their counts\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(words, counts, color='skyblue')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add a light grid\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the 30 most frequent words before preprocessing\n",
    "plot_word_frequencies(freq_dist_before, \"Top 30 Frequent Words (Before Preprocessing)\")\n",
    "\n",
    "# Plotting the 30 most frequent words after preprocessing\n",
    "plot_word_frequencies(freq_dist, \"Top 30 Frequent Words (After Preprocessing)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
